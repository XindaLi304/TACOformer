# TACOformer

Official PyTorch implementation of **Tokenâ€“Channel Compounded Cross-Attention** (TACO) for multimodal emotion recognition.

---

## ðŸ“¦ Repository Structure

tacoformer/
â”œâ”€ main.py # Entry: load data â†’ split & save test set â†’ k-fold hyperparameter search
â”‚ # â†’ train best model on training split â†’ evaluate on held-out test set
â”œâ”€ config.py # Paths and hyperparameter grid configuration
â”œâ”€ data.py # Data loading, concatenation, splitting, and DataLoader helpers
â”œâ”€ model.py # ViT + TACO cross-attention model (original functionality, English comments)
â”œâ”€ train.py # Training and evaluation loops (keeps your loss/accuracy logic)
â”œâ”€ search.py # k-fold cross-validation hyperparameter search
â”œâ”€ utils.py # Utilities (seeding, device print helpers, etc.)
â””â”€ requirements.txt # Python dependencies

---

## ðŸ§ª Data & Preprocessing

- The processed array (merged EEG/EOG/EMG) should have the shape:
[1280, 60, n_channels, 128]
where:
- `1280` = number of trials,
- `60`   = timesteps per trial,
- `n_channels` ï¼š EEGï¼š81 ï¼ŒEOGï¼š4 ï¼Œ EMGï¼š4 
- `128`  = segment length per timestep.

- Preprocessing details follow the paper:  
**TACO** â€” *Tokenâ€“Channel Compounded Cross-Attention for Multimodal Emotion Recognition*  
PDF: https://arxiv.org/pdf/2306.13592

> **Note:** This repo expects separate `.npy` files for EEG, EOG, EMG, and labels, then concatenates modalities along the channel dimension. Paths are configured in `config.py`.

---

## ðŸš€ Quick Start

```bash
# Clone and enter the project
git clone <your-repo-url>.git
cd tacoformer

# (Optional) Create & activate a virtual environment
python -m venv .venv
# macOS/Linux:
source .venv/bin/activate
# Windows (PowerShell):
# .\.venv\Scripts\Activate.ps1

# Install dependencies
pip install -r requirements.txt

# Run the full pipeline
python main.py

If you use this code, please cite the TACO paper:

Tokenâ€“Channel Compounded Cross-Attention for Multimodal Emotion Recognition, 2023.
arXiv:2306.13592
